{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvzsy-k3m-eE"
      },
      "source": [
        "# Reinforcement Learning Mouse Model of Maze Discovery\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRJ86QO3m-eG"
      },
      "source": [
        "### Importing neccesary libraries for data creation and visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lw1KMRtund8P",
        "outputId": "688455d3-278a-4321-a025-5b53b9f24915"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Z8N7O4nnk-7",
        "outputId": "ab26a9a6-5bb7-45da-bc76-668d033965a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.14.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras) (4.13.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuygndk_FToF"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, PReLU\n",
        "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
        "from random import randint\n",
        "import os, sys, time, datetime, json, random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZDFKiZAm-eI"
      },
      "source": [
        "### Code for simulating a rat in a maze with actions, agent and reward\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "twcMVfZlGkKv"
      },
      "outputs": [],
      "source": [
        "# dcreate the colors\n",
        "visited_mark = 0.8  # Cells visited by the rat will be painted by gray 0.8\n",
        "mouse_mark = 0.5  # The current rat cell will be painteg by gray 0.5\n",
        "\n",
        "# numerically assign valus to possible actions\n",
        "# assume rat cannot move diagonal\n",
        "LEFT = 0\n",
        "UP = 1\n",
        "RIGHT = 2\n",
        "DOWN = 3\n",
        "\n",
        "# Actions dictionary\n",
        "actions_dict: dict[int, str] = {\n",
        "    LEFT: \"left\",\n",
        "    UP: \"up\",\n",
        "    RIGHT: \"right\",\n",
        "    DOWN: \"down\",\n",
        "}\n",
        "\n",
        "num_actions: int = len(actions_dict)\n",
        "\n",
        "# Exploration factor\n",
        "#  one of every 10 moves the agent takes a completely random action\n",
        "epsilon: float = 1 / 10\n",
        "\n",
        "\n",
        "# maze is a 2d Numpy array of floats between 0.0 to 1.0\n",
        "# 1.0 corresponds to a free cell, and 0.0 an occupied cell\n",
        "# mouse = (row, col) initial mouse position (defaults to (0,0))\n",
        "\n",
        "\n",
        "class Qmaze(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        maze: list,\n",
        "        mouse: list = (0, 0),\n",
        "        valid_penalty: float = -0.04,\n",
        "        invalid_penality: float = -0.75,\n",
        "        visited_penality: float = -0.25,\n",
        "    ):\n",
        "        self._maze = np.array(maze)\n",
        "        nrows, ncols = self._maze.shape\n",
        "        self._valid_penality = valid_penalty\n",
        "        self._invalid_penality = invalid_penality\n",
        "        self._visited_penality = visited_penality\n",
        "\n",
        "        # target cell where the \"cheese\" is\n",
        "        # the default behaviour is that the cheese is always in the\n",
        "        # bottom right corner of the maze\n",
        "        self.target = (nrows - 1, ncols - 1)\n",
        "\n",
        "        # create free cells\n",
        "        self.free_cells = [\n",
        "            (r, c)\n",
        "            for r in range(nrows)\n",
        "            for c in range(ncols)\n",
        "            if self._maze[r, c] == 1.0\n",
        "        ]\n",
        "        # remove the target from the \"free cells\"\n",
        "        self.free_cells.remove(self.target)\n",
        "\n",
        "        # throw an exception if there is no way to get to the target cell\n",
        "        if self._maze[self.target] == 0.0:\n",
        "            raise Exception(\"Invalid maze: target cell cannot be blocked!\")\n",
        "\n",
        "        # throw an exception if the mouse is not started on a free cell\n",
        "        if not mouse in self.free_cells:\n",
        "            raise Exception(\"Invalid mouse Location: must sit on a free cell\")\n",
        "        self.reset(mouse)\n",
        "\n",
        "    def reset(self, mouse):\n",
        "        self.mouse = mouse\n",
        "        self.maze = np.copy(self._maze)\n",
        "        nrows, ncols = self.maze.shape\n",
        "        row, col = mouse\n",
        "        self.maze[row, col] = mouse_mark\n",
        "        self.state = (row, col, \"start\")\n",
        "        self.min_reward = -0.5 * self.maze.size\n",
        "        self.total_reward = 0\n",
        "        self.visited = set()\n",
        "\n",
        "    def update_state(self, action):\n",
        "        nrows, ncols = self.maze.shape\n",
        "        nrow, ncol, nmode = mouse_row, mouse_col, mode = self.state\n",
        "\n",
        "        if self.maze[mouse_row, mouse_col] > 0.0:\n",
        "            self.visited.add((mouse_row, mouse_col))  # mark visited cell\n",
        "\n",
        "        valid_actions = self.valid_actions()\n",
        "\n",
        "        if not valid_actions:\n",
        "            nmode = \"blocked\"\n",
        "        elif action in valid_actions:\n",
        "            nmode = \"valid\"\n",
        "            if action == LEFT:\n",
        "                ncol -= 1\n",
        "            elif action == UP:\n",
        "                nrow -= 1\n",
        "            if action == RIGHT:\n",
        "                ncol += 1\n",
        "            elif action == DOWN:\n",
        "                nrow += 1\n",
        "        else:  # invalid action, no change mouse position\n",
        "            mode = \"invalid\"\n",
        "\n",
        "        # new state\n",
        "        self.state = (nrow, ncol, nmode)\n",
        "\n",
        "    def get_reward(self):\n",
        "        mouse_row, mouse_col, mode = self.state\n",
        "        nrows, ncols = self.maze.shape\n",
        "        valid_penalty = self._valid_penality\n",
        "        invalid_penalty = self._invalid_penality\n",
        "        visited_penalty = self._visited_penality\n",
        "        if mouse_row == nrows - 1 and mouse_col == ncols - 1:\n",
        "            return 1.0\n",
        "        if mode == \"blocked\":\n",
        "            return self.min_reward - 1\n",
        "        if (mouse_row, mouse_col) in self.visited:\n",
        "            return visited_penalty\n",
        "        if mode == \"invalid\":\n",
        "            return invalid_penalty\n",
        "        if mode == \"valid\":\n",
        "            return valid_penalty\n",
        "\n",
        "    def act(self, action: int):\n",
        "        self.update_state(action)\n",
        "        reward = self.get_reward()\n",
        "        self.total_reward += reward\n",
        "        status = self.trial_status()\n",
        "        env_state = self.observe()\n",
        "        return env_state, reward, status\n",
        "\n",
        "    def observe(self):\n",
        "        canvas = self.create_environment()\n",
        "        env_state = canvas.reshape((1, -1))\n",
        "        return env_state\n",
        "\n",
        "    def create_environment(self):\n",
        "        canvas = np.copy(self.maze)\n",
        "        nrows, ncols = self.maze.shape\n",
        "        # clear all visual marks\n",
        "        for r in range(nrows):\n",
        "            for c in range(ncols):\n",
        "                if canvas[r, c] > 0.0:\n",
        "                    canvas[r, c] = 1.0\n",
        "        # draw the mouse\n",
        "        row, col, valid = self.state\n",
        "        canvas[row, col] = mouse_mark\n",
        "        return canvas\n",
        "\n",
        "    def trial_status(self):\n",
        "        if self.total_reward < self.min_reward:\n",
        "            return \"lose\"\n",
        "        mouse_row, mouse_col, mode = self.state\n",
        "        nrows, ncols = self.maze.shape\n",
        "        if mouse_row == nrows - 1 and mouse_col == ncols - 1:\n",
        "            return \"win\"\n",
        "        return \"not_over\"\n",
        "\n",
        "    def valid_actions(self, cell=None):\n",
        "        if cell is None:\n",
        "            row, col, _ = self.state\n",
        "        else:\n",
        "            row, col = cell\n",
        "        actions = [0, 1, 2, 3]\n",
        "        nrows, ncols = self.maze.shape\n",
        "        if row == 0:\n",
        "            actions.remove(1)\n",
        "        elif row == nrows - 1:\n",
        "            actions.remove(3)\n",
        "\n",
        "        if col == 0:\n",
        "            actions.remove(0)\n",
        "        elif col == ncols - 1:\n",
        "            actions.remove(2)\n",
        "\n",
        "        if row > 0 and self.maze[row - 1, col] == 0.0:\n",
        "            actions.remove(1)\n",
        "        if row < nrows - 1 and self.maze[row + 1, col] == 0.0:\n",
        "            actions.remove(3)\n",
        "\n",
        "        if col > 0 and self.maze[row, col - 1] == 0.0:\n",
        "            actions.remove(0)\n",
        "        if col < ncols - 1 and self.maze[row, col + 1] == 0.0:\n",
        "            actions.remove(2)\n",
        "\n",
        "        return actions\n",
        "\n",
        "\n",
        "# show 8x8 maze | WALL = BLACK | MOUSE = DARK GRAY | PATH = LIGHT GRAY | CHEESE = VERY LIGHT GRAY\n",
        "def show(qmaze: Qmaze):\n",
        "    plt.grid(\"on\")\n",
        "    nrows, ncols = qmaze.maze.shape\n",
        "    ax = plt.gca()\n",
        "    ax.set_xticks(np.arange(0.5, nrows, 1))\n",
        "    ax.set_yticks(np.arange(0.5, ncols, 1))\n",
        "    ax.set_xticklabels([])\n",
        "    ax.set_yticklabels([])\n",
        "    canvas = np.copy(qmaze.maze)\n",
        "    for row, col in qmaze.visited:\n",
        "        canvas[row, col] = 0.6\n",
        "    mouse_row, mouse_col, _ = qmaze.state\n",
        "    canvas[mouse_row, mouse_col] = 0.3  # mouse cell\n",
        "    canvas[nrows - 1, ncols - 1] = 0.9  # cheese cell\n",
        "    img = plt.imshow(canvas, interpolation=\"none\", cmap=\"gray\")\n",
        "    return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjLN198Em-eK"
      },
      "source": [
        "## Generate Random Maze\n",
        "\n",
        "Applying Depth First Search (DFS) to generate random maze based on entrance and exit (aka cheese) location adapted from https://www.geeksforgeeks.org/random-acyclic-maze-generator-with-given-entry-and-exit-point/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "u8WZ-PWIUTe3"
      },
      "outputs": [],
      "source": [
        "# Class to define structure of a node\n",
        "class Node:\n",
        "    def __init__(self, value=None, next_element=None):\n",
        "        self.val = value\n",
        "        self.next = next_element\n",
        "\n",
        "\n",
        "# Class to implement a stack\n",
        "class stack:\n",
        "    # Constructor\n",
        "    def __init__(self):\n",
        "        self.head = None\n",
        "        self.length = 0\n",
        "\n",
        "    # Put an item on the top of the stack\n",
        "    def insert(self, data):\n",
        "        self.head = Node(data, self.head)\n",
        "        self.length += 1\n",
        "\n",
        "    # Return the top position of the stack\n",
        "    def pop(self):\n",
        "        if self.length == 0:\n",
        "            return None\n",
        "        else:\n",
        "            returned = self.head.val\n",
        "            self.head = self.head.next\n",
        "            self.length -= 1\n",
        "            return returned\n",
        "\n",
        "    # Return False if the stack is empty\n",
        "    # and true otherwise\n",
        "    def not_empty(self):\n",
        "        return bool(self.length)\n",
        "\n",
        "    # Return the top position of the stack\n",
        "    def top(self):\n",
        "        return self.head.val\n",
        "\n",
        "\n",
        "def generate_random_maze(\n",
        "    rows: int = 4,\n",
        "    columns: int = 4,\n",
        "    initial_point: list = (0, 0),\n",
        "    final_point: list = (3, 3),\n",
        "):\n",
        "    ROWS, COLS = rows, columns\n",
        "\n",
        "    # Array with only walls (where paths will\n",
        "    # be created)\n",
        "    maze = list(list(0 for _ in range(COLS)) for _ in range(ROWS))\n",
        "\n",
        "    # Auxiliary matrices to avoid cycles\n",
        "    seen = list(list(False for _ in range(COLS)) for _ in range(ROWS))\n",
        "    previous = list(list((-1, -1) for _ in range(COLS)) for _ in range(ROWS))\n",
        "\n",
        "    S = stack()\n",
        "\n",
        "    # Insert initial position\n",
        "    S.insert(initial_point)\n",
        "\n",
        "    # Keep walking on the graph using dfs\n",
        "    # until we have no more paths to traverse\n",
        "    # (create)\n",
        "    while S.not_empty():\n",
        "        # Remove the position of the Stack\n",
        "        # and mark it as seen\n",
        "        x, y = S.pop()\n",
        "        seen[x][y] = True\n",
        "\n",
        "        # This is to avoid cycles with adj positions\n",
        "        if (x + 1 < ROWS) and maze[x + 1][y] == 1 and previous[x][y] != (x + 1, y):\n",
        "            continue\n",
        "        if (0 < x) and maze[x - 1][y] == 1 and previous[x][y] != (x - 1, y):\n",
        "            continue\n",
        "        if (y + 1 < COLS) and maze[x][y + 1] == 1 and previous[x][y] != (x, y + 1):\n",
        "            continue\n",
        "        if (y > 0) and maze[x][y - 1] == 1 and previous[x][y] != (x, y - 1):\n",
        "            continue\n",
        "\n",
        "        # Mark as walkable position\n",
        "        maze[x][y] = 1\n",
        "\n",
        "        # Array to shuffle neighbours before\n",
        "        # insertion\n",
        "        to_stack = []\n",
        "\n",
        "        # Before inserting any position,\n",
        "        # check if it is in the boundaries of\n",
        "        # the maze\n",
        "        # and if it were seen (to avoid cycles)\n",
        "\n",
        "        # If adj position is valid and was not seen yet\n",
        "        if (x + 1 < ROWS) and seen[x + 1][y] == False:\n",
        "            # Mark the adj position as seen\n",
        "            seen[x + 1][y] = True\n",
        "\n",
        "            # Memorize the position to insert the\n",
        "            # position in the stack\n",
        "            to_stack.append((x + 1, y))\n",
        "\n",
        "            # Memorize the current position as its\n",
        "            # previous position on the path\n",
        "            previous[x + 1][y] = (x, y)\n",
        "\n",
        "        if (0 < x) and seen[x - 1][y] == False:\n",
        "            # Mark the adj position as seen\n",
        "            seen[x - 1][y] = True\n",
        "\n",
        "            # Memorize the position to insert the\n",
        "            # position in the stack\n",
        "            to_stack.append((x - 1, y))\n",
        "\n",
        "            # Memorize the current position as its\n",
        "            # previous position on the path\n",
        "            previous[x - 1][y] = (x, y)\n",
        "\n",
        "        if (y + 1 < COLS) and seen[x][y + 1] == False:\n",
        "            # Mark the adj position as seen\n",
        "            seen[x][y + 1] = True\n",
        "\n",
        "            # Memorize the position to insert the\n",
        "            # position in the stack\n",
        "            to_stack.append((x, y + 1))\n",
        "\n",
        "            # Memorize the current position as its\n",
        "            # previous position on the path\n",
        "            previous[x][y + 1] = (x, y)\n",
        "\n",
        "        if (y > 0) and seen[x][y - 1] == False:\n",
        "            # Mark the adj position as seen\n",
        "            seen[x][y - 1] = True\n",
        "\n",
        "            # Memorize the position to insert the\n",
        "            # position in the stack\n",
        "            to_stack.append((x, y - 1))\n",
        "\n",
        "            # Memorize the current position as its\n",
        "            # previous position on the path\n",
        "            previous[x][y - 1] = (x, y)\n",
        "\n",
        "        # Indicates if Pf is a neighbour position\n",
        "        pf_flag = False\n",
        "        while len(to_stack):\n",
        "            # Remove random position\n",
        "            neighbour = to_stack.pop(randint(0, len(to_stack) - 1))\n",
        "\n",
        "            # Is the final position,\n",
        "            # remember that by marking the flag\n",
        "            if neighbour == final_point:\n",
        "                pf_flag = True\n",
        "\n",
        "            # Put on the top of the stack\n",
        "            else:\n",
        "                S.insert(neighbour)\n",
        "\n",
        "        # This way, Pf will be on the top\n",
        "        if pf_flag:\n",
        "            S.insert(final_point)\n",
        "\n",
        "    # Mark the initial position\n",
        "    x0, y0 = initial_point\n",
        "    xf, yf = final_point\n",
        "    maze[x0][y0] = 1\n",
        "    maze[xf][yf] = 1\n",
        "\n",
        "    # Return maze formed by the traversed path\n",
        "    return np.asarray(maze, dtype=\"float\")\n",
        "\n",
        "\n",
        "# Test Run to ensure that function is working correctly\n",
        "test_cols = 8\n",
        "test_rows = 8\n",
        "test_init_point = (0, 0)\n",
        "test_final_point = (7, 7)\n",
        "\n",
        "test_maze = generate_random_maze(\n",
        "    rows=test_rows,\n",
        "    columns=test_cols,\n",
        "    initial_point=test_init_point,\n",
        "    final_point=test_final_point,\n",
        ")\n",
        "# check that the shape generated is correct\n",
        "assert test_maze.shape == (test_cols, test_rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUI9ypKZm-eL"
      },
      "source": [
        "### Generating a maze array and initializing a Qmaze\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "EdFkyj_YGCEE",
        "outputId": "9dfcdcb0-fe6c-49e2-810e-6eefa7c3e3e6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7a712e3d2610>"
            ]
          },
          "metadata": {},
          "execution_count": 57
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "maze = generate_random_maze(2, 2, (0, 0), (1, 1))\n",
        "qmaze = Qmaze(maze=maze)\n",
        "show(qmaze)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maze_2 = generate_random_maze(10, 10, (0, 0), (9, 9))\n",
        "qmaze_2 = Qmaze(maze=maze_2)\n",
        "show(qmaze_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "F5cEG8rz7kUL",
        "outputId": "93f651d1-8c06-4d2e-ada4-29137bf3a0a8"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7a712e1b2450>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "class BiologicallyPlausibleModel(nn.Module):\n",
        "    def __init__(self, maze_size, num_actions, learning_rate=0.001):\n",
        "        super(BiologicallyPlausibleModel, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(maze_size, maze_size),\n",
        "            nn.PReLU(),\n",
        "            nn.Linear(maze_size, maze_size),\n",
        "            nn.PReLU(),\n",
        "            nn.Linear(maze_size, num_actions)\n",
        "        )\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "        # Only dopamine\n",
        "        self.dopamine_level = 1.0\n",
        "\n",
        "    def forward(self, state):\n",
        "        return self.model(state)\n",
        "\n",
        "    def update_reward(self, reward, expected_reward):\n",
        "        # Only Dopamine modulates reward prediction error\n",
        "        dopamine_signal = reward - expected_reward\n",
        "        self.dopamine_level += dopamine_signal * 0.1  # Adjust dopamine level\n",
        "        return reward\n",
        "\n",
        "    def train(self, state, action, reward, next_state):\n",
        "        Q_values = self.model(state)\n",
        "        with torch.no_grad():\n",
        "            next_Q_values = self.model(next_state)\n",
        "            Q_target = reward * self.dopamine_level + torch.max(next_Q_values)\n",
        "\n",
        "        loss = (Q_values[0, action] - Q_target) ** 2\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss  # ✅ return loss for tracking\n"
      ],
      "metadata": {
        "id": "7-pwoPAHsBKn"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "8TPZpNAvm-eN"
      },
      "outputs": [],
      "source": [
        "class Model(object):\n",
        "    def __init__(self, maze: list, learning_rate: float = 0.001):\n",
        "        self.maze_size = maze.size\n",
        "        self.model = BiologicallyPlausibleModel(self.maze_size, num_actions, learning_rate)\n",
        "\n",
        "    def get_model(self):\n",
        "        return self.model\n",
        "\n",
        "    def predict(self, env_state):\n",
        "        env_state = torch.tensor(env_state, dtype=torch.float32)\n",
        "        return self.model.forward(env_state).detach().numpy()[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlxyULjym-eN"
      },
      "source": [
        "## Create a Trial\n",
        "\n",
        "Create an `Trial` class that accepts a trained neural network which calculates the next action, a Qmaze and the initial cell that the mouse is in.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "yr5j0qwsm-eN"
      },
      "outputs": [],
      "source": [
        "class Trial:\n",
        "    def __init__(self, model: Model, qmaze: Qmaze, mouse_cell: list):\n",
        "        self._qmaze = qmaze\n",
        "        self._model = model\n",
        "        self.mouse_cell = mouse_cell\n",
        "\n",
        "    def run(self):\n",
        "        print(\"running trial...\")\n",
        "\n",
        "        mouse_cell = self._get_mouse_cell()\n",
        "        self._qmaze.reset(mouse_cell)\n",
        "        env_state = self._qmaze.observe()\n",
        "\n",
        "        while True:\n",
        "            prev_env_state = env_state\n",
        "            Q_values = self._model.get_model().forward(torch.tensor(prev_env_state, dtype=torch.float32)).detach().numpy()[0]\n",
        "            action = np.argmax(Q_values)\n",
        "\n",
        "            _, _, status = self._qmaze.act(action)\n",
        "\n",
        "            if status == \"win\":\n",
        "                return True\n",
        "            elif status == \"lose\":\n",
        "                return False\n",
        "\n",
        "    def check(self):\n",
        "        self._qmaze = self._get_maze()\n",
        "\n",
        "        for cell in self._qmaze.free_cells:\n",
        "            if not self._qmaze.valid_actions(cell):\n",
        "                return False\n",
        "\n",
        "            if not self.run():\n",
        "                return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _get_maze(self):\n",
        "        return self._qmaze\n",
        "\n",
        "    def _get_model(self):\n",
        "        return self._model\n",
        "\n",
        "    def _get_mouse_cell(self):\n",
        "        return self.mouse_cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9ZTvOn2m-eO"
      },
      "source": [
        "## Creating a Class to Model the Experience of the Mouse\n",
        "\n",
        "Create an `Experience` class that collects the experience of `Experiments` within a `list` of memory. It retreives a `model`, a `max_memory` which defines the maximum amount of experiments that the mouse can _remember_ and a `discount` factor which represents the instantanious uncertainty in the _Bellman equation for stochastic environments_.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "ZrZb8y5Em-eP"
      },
      "outputs": [],
      "source": [
        "class Experience(object):\n",
        "    def __init__(self, model: Model, max_memory: int = 8, discount: float = 95 / 100):\n",
        "        self.model = model\n",
        "        self.max_memory = max_memory\n",
        "        self.discount = discount\n",
        "        self.memory = list()\n",
        "        self.actions = model.get_model().model[-1].out_features  # Access num_actions from the model\n",
        "\n",
        "    def remember(self, trial):\n",
        "        self.memory.append(trial)\n",
        "        if len(self.memory) > self.max_memory:\n",
        "            # delete the first element of the memory list if we exceed the max memory\n",
        "            del self.memory[0]\n",
        "\n",
        "    def predict(self, env_state):\n",
        "        env_state = torch.tensor(env_state, dtype=torch.float32)\n",
        "        return self.model.get_model().forward(env_state).detach().numpy()[0]\n",
        "\n",
        "    def data(self, data_size: int = 10):\n",
        "        environment_size = self.memory[0][0].shape[1]\n",
        "        memory_size = len(self.memory)\n",
        "        data_size = min(memory_size, data_size)\n",
        "\n",
        "        inputs = np.zeros((data_size, environment_size))\n",
        "        targets = np.zeros((data_size, self.actions))\n",
        "\n",
        "        for idx, jdx in enumerate(\n",
        "            np.random.choice(range(memory_size), data_size, replace=False)\n",
        "        ):\n",
        "            env_state, action, reward, env_state_next, trial_over = self.memory[jdx]\n",
        "\n",
        "            inputs[idx] = env_state\n",
        "            targets[idx] = self.predict(env_state)\n",
        "\n",
        "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
        "            Q_sa = np.max(self.predict(env_state_next))\n",
        "\n",
        "            if trial_over:\n",
        "                targets[idx, action] = reward\n",
        "            else:\n",
        "                # reward + gamma * max_a' Q(s', a')\n",
        "                targets[idx, action] = reward + self.discount * Q_sa\n",
        "\n",
        "        return inputs, targets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XGfycrxm-eP"
      },
      "source": [
        "# Q-Training Algorithm for Reinforcement Learning of Mouse\n",
        "The algorithm accepts the a `number_epoch` which is the number of epochs, the maximum memory `max_memory` which is the maximum number of trials kept in memory and the `data_size` which is the number of  samples in training epoch. This is the number of trials randomly selected from the mouse's experience"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "Zt7kUU8wm-eP"
      },
      "outputs": [],
      "source": [
        "# This is a small utility for printing readable time strings:\n",
        "def format_time(seconds):\n",
        "    if seconds < 400:\n",
        "        s = float(seconds)\n",
        "        return \"%.1f seconds\" % (s,)\n",
        "    elif seconds < 4000:\n",
        "        m = seconds / 60.0\n",
        "        return \"%.2f minutes\" % (m,)\n",
        "    else:\n",
        "        h = seconds / 3600.0\n",
        "        return \"%.2f hours\" % (h,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "hhu36ts-m-eQ"
      },
      "outputs": [],
      "source": [
        "class Experiment(object):\n",
        "    def __init__(self, maze=generate_random_maze(8, 8, (0,0), (7,7)), model_learning_rate: float = 0.001):\n",
        "        qmaze = Qmaze(maze)\n",
        "        model = Model(maze, learning_rate=model_learning_rate)\n",
        "        trial = Trial(model, qmaze, (0, 0))\n",
        "        self.qmaze = qmaze\n",
        "        self.model = model\n",
        "        self.trial = trial\n",
        "\n",
        "    def train(self, **opt):\n",
        "        show(self.qmaze)\n",
        "        print(\"training....\")\n",
        "\n",
        "        global epsilon\n",
        "        number_epoch = opt.get(\"epochs\", 100)\n",
        "        max_memory = opt.get(\"max_memory\", 8)\n",
        "        data_size = opt.get(\"data_size\", 25)\n",
        "        name = opt.get(\"name\", \"model\")\n",
        "\n",
        "        start_time = datetime.datetime.now()\n",
        "\n",
        "        # Initialize experience replay object\n",
        "        experience = Experience(self.model, max_memory=max_memory)\n",
        "\n",
        "        completion_history = [] # history of win/lose game\n",
        "        number_free_cells = len(self.qmaze.free_cells)\n",
        "        hsize = self.qmaze.maze.size // 2 # history window size\n",
        "        win_rate = 0.0\n",
        "        imctr = 1\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for epoch in range(number_epoch):\n",
        "            loss = 0.0\n",
        "            mouse_cell = random.choice(self.qmaze.free_cells)\n",
        "            self.qmaze.reset(mouse_cell)\n",
        "            trial_over = False\n",
        "\n",
        "            # get initial env_state (1d flattened canvas)\n",
        "            env_state = self.qmaze.observe()\n",
        "\n",
        "            n_trials = 0\n",
        "            while not trial_over:\n",
        "                valid_actions = self.qmaze.valid_actions()\n",
        "                if not valid_actions:\n",
        "                    break\n",
        "\n",
        "                prev_env_state = env_state\n",
        "\n",
        "                # Get next action\n",
        "                if np.random.rand() < epsilon:\n",
        "                    action = random.choice(valid_actions)\n",
        "                else:\n",
        "                    Q_values = self.model.get_model().forward(torch.tensor(prev_env_state, dtype=torch.float32)).detach().numpy()[0]\n",
        "                    action = np.argmax(Q_values)\n",
        "\n",
        "                # Apply action, get reward and new env_state\n",
        "                env_state, reward, status = self.qmaze.act(action)\n",
        "\n",
        "                if status == \"win\":\n",
        "                    completion_history.append(1)\n",
        "                    trial_over = True\n",
        "                elif status == \"lose\":\n",
        "                    completion_history.append(0)\n",
        "                    trial_over = True\n",
        "                else:\n",
        "                    trial_over = False\n",
        "\n",
        "                # Store trial (experience)\n",
        "                trial = [prev_env_state, action, reward, env_state, trial_over]\n",
        "                experience.remember(trial)\n",
        "\n",
        "                n_trials += 1\n",
        "\n",
        "                # Train neural network model\n",
        "                inputs, targets = experience.data(data_size=data_size)\n",
        "\n",
        "                # Convert inputs and targets to tensors if necessary\n",
        "                inputs = torch.tensor(inputs, dtype=torch.float32)\n",
        "                targets = torch.tensor(targets, dtype=torch.float32)\n",
        "\n",
        "                # Use the train method instead of fit\n",
        "                for idx, action in enumerate(np.argmax(targets, axis=1)):\n",
        "                    state = inputs[idx].unsqueeze(0)  # Add batch dimension\n",
        "                    reward = targets[idx, action].item()  # Extract reward for the action\n",
        "                    next_state = inputs[idx].unsqueeze(0)  # Use current state as next state for simplicity\n",
        "\n",
        "                        # Train and accumulate loss\n",
        "                    loss = self.model.get_model().train(state, action, reward, next_state)\n",
        "                    epoch_loss += loss.item()\n",
        "                    #self.model.get_model().train(state, action, reward, next_state)\n",
        "\n",
        "                # Update dopamine level\n",
        "                expected_reward = np.max(self.model.get_model().forward(torch.tensor(prev_env_state, dtype=torch.float32)).detach().numpy()[0])\n",
        "                self.model.get_model().update_reward(reward, expected_reward)\n",
        "\n",
        "            if len(completion_history) > hsize:\n",
        "                win_rate = sum(completion_history[-hsize:]) / hsize\n",
        "\n",
        "            dt = datetime.datetime.now() - start_time\n",
        "            t = format_time(dt.total_seconds())\n",
        "\n",
        "            template = \"Epoch: {:03d}/{:d} | Trials: {:d} | Win count: {:d} | Win rate: {:.3f} | Loss: {:.4f} | time: {}\"\n",
        "            print(\n",
        "                template.format(\n",
        "                    epoch,\n",
        "                    number_epoch - 1,\n",
        "                    n_trials,\n",
        "                    sum(completion_history),\n",
        "                    win_rate,\n",
        "                    epoch_loss,  # include loss\n",
        "                    t,\n",
        "                ),\n",
        "            )\n",
        "\n",
        "            # we simply check if training has exhausted all free cells and if in all\n",
        "            # cases the agent won\n",
        "            if win_rate > 0.9:\n",
        "                epsilon = 0.05\n",
        "            if sum(completion_history[-hsize:]) == hsize: # and self.trial.check():\n",
        "                print(\"Reached 100%% win rate at epoch: %d\" % (epoch,))\n",
        "                break\n",
        "\n",
        "        end_time = datetime.datetime.now()\n",
        "        dt = datetime.datetime.now() - start_time\n",
        "        seconds = dt.total_seconds()\n",
        "        t = format_time(seconds)\n",
        "\n",
        "        print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (epoch, max_memory, data_size, t))\n",
        "\n",
        "        return seconds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HTDDyLRm-eQ"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2gTtl_9pm-eR",
        "outputId": "011587f8-e4d7-4489-c461-f238d0da9f80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training....\n",
            "Epoch: 000/99 | Trials: 134 | Win count: 0 | Win rate: 0.000 | Loss: 490.2685 | time: 2.1 seconds\n",
            "Epoch: 001/99 | Trials: 134 | Win count: 0 | Win rate: 0.000 | Loss: 490.7167 | time: 4.3 seconds\n",
            "Epoch: 002/99 | Trials: 133 | Win count: 0 | Win rate: 0.000 | Loss: 491.3201 | time: 6.5 seconds\n",
            "Epoch: 003/99 | Trials: 132 | Win count: 0 | Win rate: 0.000 | Loss: 491.4315 | time: 8.6 seconds\n",
            "Epoch: 004/99 | Trials: 132 | Win count: 0 | Win rate: 0.000 | Loss: 491.8169 | time: 11.0 seconds\n",
            "Epoch: 005/99 | Trials: 130 | Win count: 0 | Win rate: 0.000 | Loss: 492.5672 | time: 13.5 seconds\n",
            "Epoch: 006/99 | Trials: 132 | Win count: 0 | Win rate: 0.000 | Loss: 492.8017 | time: 15.6 seconds\n",
            "Epoch: 007/99 | Trials: 132 | Win count: 0 | Win rate: 0.000 | Loss: 493.0175 | time: 17.7 seconds\n",
            "Epoch: 008/99 | Trials: 134 | Win count: 0 | Win rate: 0.000 | Loss: 493.3328 | time: 19.8 seconds\n",
            "Epoch: 009/99 | Trials: 134 | Win count: 0 | Win rate: 0.000 | Loss: 513.4219 | time: 22.0 seconds\n",
            "Epoch: 010/99 | Trials: 130 | Win count: 0 | Win rate: 0.000 | Loss: 745.2382 | time: 24.6 seconds\n",
            "Epoch: 011/99 | Trials: 137 | Win count: 0 | Win rate: 0.000 | Loss: 745.2413 | time: 27.1 seconds\n",
            "Epoch: 012/99 | Trials: 132 | Win count: 0 | Win rate: 0.000 | Loss: 745.2428 | time: 29.2 seconds\n",
            "Epoch: 013/99 | Trials: 131 | Win count: 0 | Win rate: 0.000 | Loss: 745.2711 | time: 31.2 seconds\n",
            "Epoch: 014/99 | Trials: 135 | Win count: 0 | Win rate: 0.000 | Loss: 745.6351 | time: 33.3 seconds\n",
            "Epoch: 015/99 | Trials: 135 | Win count: 0 | Win rate: 0.000 | Loss: 745.9868 | time: 35.6 seconds\n",
            "Epoch: 016/99 | Trials: 131 | Win count: 0 | Win rate: 0.000 | Loss: 746.1860 | time: 38.2 seconds\n",
            "Epoch: 017/99 | Trials: 134 | Win count: 0 | Win rate: 0.000 | Loss: 746.3514 | time: 40.3 seconds\n",
            "Epoch: 018/99 | Trials: 132 | Win count: 0 | Win rate: 0.000 | Loss: 746.4661 | time: 42.4 seconds\n",
            "Epoch: 019/99 | Trials: 134 | Win count: 0 | Win rate: 0.000 | Loss: 746.5512 | time: 44.5 seconds\n",
            "Epoch: 020/99 | Trials: 130 | Win count: 0 | Win rate: 0.000 | Loss: 746.5518 | time: 46.5 seconds\n",
            "Epoch: 021/99 | Trials: 130 | Win count: 0 | Win rate: 0.000 | Loss: 746.6168 | time: 48.9 seconds\n",
            "Epoch: 022/99 | Trials: 136 | Win count: 0 | Win rate: 0.000 | Loss: 746.6557 | time: 51.4 seconds\n",
            "Epoch: 023/99 | Trials: 134 | Win count: 0 | Win rate: 0.000 | Loss: 763.2830 | time: 53.5 seconds\n",
            "Epoch: 024/99 | Trials: 137 | Win count: 0 | Win rate: 0.000 | Loss: 763.2833 | time: 55.7 seconds\n",
            "Epoch: 025/99 | Trials: 138 | Win count: 0 | Win rate: 0.000 | Loss: 763.2858 | time: 57.9 seconds\n",
            "Epoch: 026/99 | Trials: 138 | Win count: 0 | Win rate: 0.000 | Loss: 763.4459 | time: 60.3 seconds\n",
            "Epoch: 027/99 | Trials: 139 | Win count: 0 | Win rate: 0.000 | Loss: 763.5380 | time: 63.0 seconds\n",
            "Epoch: 028/99 | Trials: 131 | Win count: 0 | Win rate: 0.000 | Loss: 763.5795 | time: 65.1 seconds\n",
            "Epoch: 029/99 | Trials: 138 | Win count: 0 | Win rate: 0.000 | Loss: 763.6762 | time: 67.3 seconds\n",
            "Epoch: 030/99 | Trials: 139 | Win count: 0 | Win rate: 0.000 | Loss: 763.7059 | time: 69.5 seconds\n",
            "Epoch: 031/99 | Trials: 129 | Win count: 0 | Win rate: 0.000 | Loss: 763.7496 | time: 71.6 seconds\n",
            "Epoch: 032/99 | Trials: 138 | Win count: 0 | Win rate: 0.000 | Loss: 763.7991 | time: 74.3 seconds\n",
            "Epoch: 033/99 | Trials: 135 | Win count: 0 | Win rate: 0.000 | Loss: 763.7993 | time: 76.7 seconds\n",
            "Epoch: 034/99 | Trials: 130 | Win count: 0 | Win rate: 0.000 | Loss: 763.8298 | time: 78.7 seconds\n",
            "Epoch: 035/99 | Trials: 129 | Win count: 0 | Win rate: 0.000 | Loss: 763.8433 | time: 80.9 seconds\n",
            "Epoch: 036/99 | Trials: 139 | Win count: 0 | Win rate: 0.000 | Loss: 771.2050 | time: 83.2 seconds\n",
            "Epoch: 037/99 | Trials: 133 | Win count: 0 | Win rate: 0.000 | Loss: 771.2069 | time: 85.7 seconds\n",
            "Epoch: 038/99 | Trials: 131 | Win count: 0 | Win rate: 0.000 | Loss: 771.2873 | time: 88.2 seconds\n",
            "Epoch: 039/99 | Trials: 136 | Win count: 0 | Win rate: 0.000 | Loss: 771.3422 | time: 90.3 seconds\n",
            "Epoch: 040/99 | Trials: 133 | Win count: 0 | Win rate: 0.000 | Loss: 771.3756 | time: 92.4 seconds\n",
            "Epoch: 041/99 | Trials: 137 | Win count: 0 | Win rate: 0.000 | Loss: 771.4044 | time: 94.5 seconds\n",
            "Epoch: 042/99 | Trials: 139 | Win count: 0 | Win rate: 0.000 | Loss: 771.4069 | time: 96.7 seconds\n",
            "Epoch: 043/99 | Trials: 132 | Win count: 0 | Win rate: 0.000 | Loss: 771.4331 | time: 99.3 seconds\n",
            "Epoch: 044/99 | Trials: 133 | Win count: 0 | Win rate: 0.000 | Loss: 771.4596 | time: 101.6 seconds\n",
            "Epoch: 045/99 | Trials: 139 | Win count: 0 | Win rate: 0.000 | Loss: 832.6881 | time: 103.8 seconds\n",
            "Epoch: 046/99 | Trials: 139 | Win count: 0 | Win rate: 0.000 | Loss: 832.6882 | time: 106.0 seconds\n",
            "Epoch: 047/99 | Trials: 130 | Win count: 0 | Win rate: 0.000 | Loss: 832.6883 | time: 108.0 seconds\n",
            "Epoch: 048/99 | Trials: 138 | Win count: 0 | Win rate: 0.000 | Loss: 832.7208 | time: 110.4 seconds\n",
            "Epoch: 049/99 | Trials: 130 | Win count: 0 | Win rate: 0.000 | Loss: 833.0280 | time: 112.9 seconds\n",
            "Epoch: 050/99 | Trials: 140 | Win count: 0 | Win rate: 0.000 | Loss: 833.0845 | time: 115.1 seconds\n",
            "Epoch: 051/99 | Trials: 139 | Win count: 0 | Win rate: 0.000 | Loss: 833.3858 | time: 117.4 seconds\n",
            "Epoch: 052/99 | Trials: 131 | Win count: 0 | Win rate: 0.000 | Loss: 833.5357 | time: 119.4 seconds\n",
            "Epoch: 053/99 | Trials: 132 | Win count: 0 | Win rate: 0.000 | Loss: 833.5367 | time: 121.7 seconds\n",
            "Epoch: 054/99 | Trials: 139 | Win count: 0 | Win rate: 0.000 | Loss: 833.6364 | time: 124.6 seconds\n",
            "Epoch: 055/99 | Trials: 135 | Win count: 0 | Win rate: 0.000 | Loss: 833.6699 | time: 126.8 seconds\n",
            "Epoch: 056/99 | Trials: 135 | Win count: 0 | Win rate: 0.000 | Loss: 833.7004 | time: 128.9 seconds\n",
            "Epoch: 057/99 | Trials: 136 | Win count: 0 | Win rate: 0.000 | Loss: 833.7274 | time: 131.0 seconds\n",
            "Epoch: 058/99 | Trials: 139 | Win count: 0 | Win rate: 0.000 | Loss: 833.7481 | time: 133.3 seconds\n",
            "Epoch: 059/99 | Trials: 135 | Win count: 0 | Win rate: 0.000 | Loss: 833.7755 | time: 135.9 seconds\n",
            "Epoch: 060/99 | Trials: 137 | Win count: 0 | Win rate: 0.000 | Loss: 837.5579 | time: 138.7 seconds\n",
            "Epoch: 061/99 | Trials: 130 | Win count: 0 | Win rate: 0.000 | Loss: 837.5579 | time: 140.8 seconds\n",
            "Epoch: 062/99 | Trials: 135 | Win count: 0 | Win rate: 0.000 | Loss: 837.5579 | time: 143.0 seconds\n",
            "Epoch: 063/99 | Trials: 131 | Win count: 0 | Win rate: 0.000 | Loss: 837.5579 | time: 145.0 seconds\n",
            "Epoch: 064/99 | Trials: 134 | Win count: 0 | Win rate: 0.000 | Loss: 837.5637 | time: 147.5 seconds\n",
            "Epoch: 065/99 | Trials: 132 | Win count: 0 | Win rate: 0.000 | Loss: 847.3318 | time: 150.2 seconds\n",
            "Epoch: 066/99 | Trials: 132 | Win count: 0 | Win rate: 0.000 | Loss: 847.3319 | time: 152.2 seconds\n",
            "Epoch: 067/99 | Trials: 130 | Win count: 0 | Win rate: 0.000 | Loss: 847.3319 | time: 154.3 seconds\n",
            "Epoch: 068/99 | Trials: 132 | Win count: 0 | Win rate: 0.000 | Loss: 847.3896 | time: 156.3 seconds\n",
            "Epoch: 069/99 | Trials: 129 | Win count: 0 | Win rate: 0.000 | Loss: 847.4218 | time: 158.3 seconds\n",
            "Epoch: 070/99 | Trials: 130 | Win count: 0 | Win rate: 0.000 | Loss: 847.4742 | time: 160.9 seconds\n",
            "Epoch: 071/99 | Trials: 135 | Win count: 0 | Win rate: 0.000 | Loss: 847.5175 | time: 163.4 seconds\n",
            "Epoch: 072/99 | Trials: 131 | Win count: 0 | Win rate: 0.000 | Loss: 847.5568 | time: 165.5 seconds\n",
            "Epoch: 073/99 | Trials: 135 | Win count: 0 | Win rate: 0.000 | Loss: 847.6103 | time: 167.7 seconds\n",
            "Epoch: 074/99 | Trials: 139 | Win count: 0 | Win rate: 0.000 | Loss: 847.6135 | time: 169.8 seconds\n",
            "Epoch: 075/99 | Trials: 12 | Win count: 1 | Win rate: 0.031 | Loss: 852.9838 | time: 170.0 seconds\n",
            "Epoch: 076/99 | Trials: 133 | Win count: 1 | Win rate: 0.031 | Loss: 916.2096 | time: 172.1 seconds\n",
            "Epoch: 077/99 | Trials: 135 | Win count: 1 | Win rate: 0.031 | Loss: 916.2114 | time: 175.0 seconds\n",
            "Epoch: 078/99 | Trials: 5 | Win count: 2 | Win rate: 0.062 | Loss: 923.3582 | time: 175.1 seconds\n",
            "Epoch: 079/99 | Trials: 135 | Win count: 2 | Win rate: 0.062 | Loss: 1160.9068 | time: 177.3 seconds\n",
            "Epoch: 080/99 | Trials: 138 | Win count: 2 | Win rate: 0.062 | Loss: 1582.8408 | time: 179.4 seconds\n",
            "Epoch: 081/99 | Trials: 139 | Win count: 2 | Win rate: 0.062 | Loss: 1621.4065 | time: 181.7 seconds\n",
            "Epoch: 082/99 | Trials: 133 | Win count: 2 | Win rate: 0.062 | Loss: 1621.4439 | time: 183.7 seconds\n",
            "Epoch: 083/99 | Trials: 132 | Win count: 2 | Win rate: 0.062 | Loss: 1623.7963 | time: 186.4 seconds\n",
            "Epoch: 084/99 | Trials: 134 | Win count: 2 | Win rate: 0.062 | Loss: 1624.7303 | time: 188.7 seconds\n",
            "Epoch: 085/99 | Trials: 135 | Win count: 2 | Win rate: 0.062 | Loss: 1625.0092 | time: 190.8 seconds\n",
            "Epoch: 086/99 | Trials: 139 | Win count: 2 | Win rate: 0.062 | Loss: 1625.4436 | time: 193.0 seconds\n",
            "Epoch: 087/99 | Trials: 131 | Win count: 2 | Win rate: 0.062 | Loss: 1625.8119 | time: 195.1 seconds\n",
            "Epoch: 088/99 | Trials: 139 | Win count: 2 | Win rate: 0.062 | Loss: 1626.2476 | time: 197.5 seconds\n",
            "Epoch: 089/99 | Trials: 136 | Win count: 2 | Win rate: 0.062 | Loss: 1681.1280 | time: 200.3 seconds\n",
            "Epoch: 090/99 | Trials: 130 | Win count: 2 | Win rate: 0.062 | Loss: 1681.4323 | time: 202.3 seconds\n",
            "Epoch: 091/99 | Trials: 135 | Win count: 2 | Win rate: 0.062 | Loss: 1681.5014 | time: 204.4 seconds\n",
            "Epoch: 092/99 | Trials: 130 | Win count: 2 | Win rate: 0.062 | Loss: 1681.6529 | time: 206.6 seconds\n",
            "Epoch: 093/99 | Trials: 133 | Win count: 2 | Win rate: 0.062 | Loss: 1681.8803 | time: 208.8 seconds\n",
            "Epoch: 094/99 | Trials: 133 | Win count: 2 | Win rate: 0.062 | Loss: 1682.1314 | time: 211.8 seconds\n",
            "Epoch: 095/99 | Trials: 134 | Win count: 2 | Win rate: 0.062 | Loss: 1682.6057 | time: 213.9 seconds\n",
            "Epoch: 096/99 | Trials: 134 | Win count: 2 | Win rate: 0.062 | Loss: 1683.0337 | time: 216.2 seconds\n",
            "Epoch: 097/99 | Trials: 135 | Win count: 2 | Win rate: 0.062 | Loss: 1683.3384 | time: 218.3 seconds\n",
            "Epoch: 098/99 | Trials: 136 | Win count: 2 | Win rate: 0.062 | Loss: 3009.7420 | time: 220.5 seconds\n",
            "Epoch: 099/99 | Trials: 134 | Win count: 2 | Win rate: 0.062 | Loss: 3009.8298 | time: 223.1 seconds\n",
            "n_epoch: 99, max_mem: 8, data: 25, time: 223.1 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "223.075555"
            ]
          },
          "metadata": {},
          "execution_count": 135
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "experiment = Experiment()\n",
        "\n",
        "experiment.train()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}