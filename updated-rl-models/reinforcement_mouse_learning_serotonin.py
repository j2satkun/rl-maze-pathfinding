# -*- coding: utf-8 -*-
"""reinforcement-mouse-learning_SEROTONIN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L573A3EkoBY3zJmkF0VuavIINTeRwM2F

# Reinforcement Learning Mouse Model of Maze Discovery

### Importing neccesary libraries for data creation and visualization
"""
import csv
from collections import deque
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
from random import randint
import os, sys, time, datetime, json, random

"""### Code for simulating a rat in a maze with actions, agent and reward

"""

# dcreate the colors
visited_mark = 0.8  # Cells visited by the rat will be painted by gray 0.8
mouse_mark = 0.5  # The current rat cell will be painteg by gray 0.5

# numerically assign valus to possible actions
# assume rat cannot move diagonal
LEFT = 0
UP = 1
RIGHT = 2
DOWN = 3

# Actions dictionary
actions_dict: dict[int, str] = {
    LEFT: "left",
    UP: "up",
    RIGHT: "right",
    DOWN: "down",
}

num_actions: int = len(actions_dict)

# Exploration factor
#  one of every 10 moves the agent takes a completely random action
epsilon: float = 1 / 10


# maze is a 2d Numpy array of floats between 0.0 to 1.0
# 1.0 corresponds to a free cell, and 0.0 an occupied cell
# mouse = (row, col) initial mouse position (defaults to (0,0))


class Qmaze(object):
    def __init__(
        self,
        maze: list,
        mouse: list = (0, 0),
        valid_penalty: float = -0.04,
        invalid_penality: float = -0.75,
        visited_penality: float = -0.25,
    ):
        self._maze = np.array(maze)
        nrows, ncols = self._maze.shape
        self._valid_penality = valid_penalty
        self._invalid_penality = invalid_penality
        self._visited_penality = visited_penality

        # target cell where the "cheese" is
        # the default behaviour is that the cheese is always in the
        # bottom right corner of the maze
        self.target = (nrows - 1, ncols - 1)

        # create free cells
        self.free_cells = [
            (r, c)
            for r in range(nrows)
            for c in range(ncols)
            if self._maze[r, c] == 1.0
        ]
        # remove the target from the "free cells"
        self.free_cells.remove(self.target)

        # throw an exception if there is no way to get to the target cell
        if self._maze[self.target] == 0.0:
            raise Exception("Invalid maze: target cell cannot be blocked!")

        # throw an exception if the mouse is not started on a free cell
        if not mouse in self.free_cells:
            raise Exception("Invalid mouse Location: must sit on a free cell")
        self.reset(mouse)

    def reset(self, mouse):
        self.mouse = mouse
        self.maze = np.copy(self._maze)
        nrows, ncols = self.maze.shape
        row, col = mouse
        self.maze[row, col] = mouse_mark
        self.state = (row, col, "start")
        self.min_reward = -0.5 * self.maze.size
        self.total_reward = 0
        self.visited = set()

    def update_state(self, action):
        nrows, ncols = self.maze.shape
        nrow, ncol, nmode = mouse_row, mouse_col, mode = self.state

        if self.maze[mouse_row, mouse_col] > 0.0:
            self.visited.add((mouse_row, mouse_col))  # mark visited cell

        valid_actions = self.valid_actions()

        if not valid_actions:
            nmode = "blocked"
        elif action in valid_actions:
            nmode = "valid"
            if action == LEFT:
                ncol -= 1
            elif action == UP:
                nrow -= 1
            if action == RIGHT:
                ncol += 1
            elif action == DOWN:
                nrow += 1
        else:  # invalid action, no change mouse position
            mode = "invalid"

        # new state
        self.state = (nrow, ncol, nmode)

    def get_reward(self):
        mouse_row, mouse_col, mode = self.state
        nrows, ncols = self.maze.shape
        valid_penalty = self._valid_penality
        invalid_penalty = self._invalid_penality
        visited_penalty = self._visited_penality
        if mouse_row == nrows - 1 and mouse_col == ncols - 1:
            return 1.0
        if mode == "blocked":
            return self.min_reward - 1
        if (mouse_row, mouse_col) in self.visited:
            return visited_penalty
        if mode == "invalid":
            return invalid_penalty
        if mode == "valid":
            return valid_penalty

    def act(self, action: int):
        self.update_state(action)
        reward = self.get_reward()
        self.total_reward += reward
        status = self.trial_status()
        env_state = self.observe()
        return env_state, reward, status

    def observe(self):
        canvas = self.create_environment()
        env_state = canvas.reshape((1, -1))
        return env_state

    def create_environment(self):
        canvas = np.copy(self.maze)
        nrows, ncols = self.maze.shape
        # clear all visual marks
        for r in range(nrows):
            for c in range(ncols):
                if canvas[r, c] > 0.0:
                    canvas[r, c] = 1.0
        # draw the mouse
        row, col, valid = self.state
        canvas[row, col] = mouse_mark
        return canvas

    def trial_status(self):
        if self.total_reward < self.min_reward:
            return "lose"
        mouse_row, mouse_col, mode = self.state
        nrows, ncols = self.maze.shape
        if mouse_row == nrows - 1 and mouse_col == ncols - 1:
            return "win"
        return "not_over"

    def valid_actions(self, cell=None):
        if cell is None:
            row, col, _ = self.state
        else:
            row, col = cell
        actions = [0, 1, 2, 3]
        nrows, ncols = self.maze.shape
        if row == 0:
            actions.remove(1)
        elif row == nrows - 1:
            actions.remove(3)

        if col == 0:
            actions.remove(0)
        elif col == ncols - 1:
            actions.remove(2)

        if row > 0 and self.maze[row - 1, col] == 0.0:
            actions.remove(1)
        if row < nrows - 1 and self.maze[row + 1, col] == 0.0:
            actions.remove(3)

        if col > 0 and self.maze[row, col - 1] == 0.0:
            actions.remove(0)
        if col < ncols - 1 and self.maze[row, col + 1] == 0.0:
            actions.remove(2)

        return actions

    def shortest_path(self, start=None, goal=None):
        """Find shortest path from start to goal using BFS."""
        if start is None:
            row, col, _ = self.state
            start = (row, col)
        if goal is None:
            goal = self.target

        queue = deque()
        queue.append((start, [start]))  # (current_pos, path_so_far)
        visited = set()
        visited.add(start)

        while queue:
            current, path = queue.popleft()
            if current == goal:
                return path

            for action in self.valid_actions(current):
                r, c = current
                if action == LEFT:
                    neighbor = (r, c - 1)
                elif action == RIGHT:
                    neighbor = (r, c + 1)
                elif action == UP:
                    neighbor = (r - 1, c)
                elif action == DOWN:
                    neighbor = (r + 1, c)
                else:
                    continue

                if neighbor not in visited:
                    visited.add(neighbor)
                    queue.append((neighbor, path + [neighbor]))

        return None  # No path found


# show 8x8 maze | WALL = BLACK | MOUSE = DARK GRAY | PATH = LIGHT GRAY | CHEESE = VERY LIGHT GRAY
def show(qmaze: Qmaze):
    plt.grid("on")
    nrows, ncols = qmaze.maze.shape
    ax = plt.gca()
    ax.set_xticks(np.arange(0.5, nrows, 1))
    ax.set_yticks(np.arange(0.5, ncols, 1))
    ax.set_xticklabels([])
    ax.set_yticklabels([])
    canvas = np.copy(qmaze.maze)
    for row, col in qmaze.visited:
        canvas[row, col] = 0.6
    mouse_row, mouse_col, _ = qmaze.state
    canvas[mouse_row, mouse_col] = 0.3  # mouse cell
    canvas[nrows - 1, ncols - 1] = 0.9  # cheese cell
    img = plt.imshow(canvas, interpolation="none", cmap="gray")
    return img

"""## Generate Random Maze

Applying Depth First Search (DFS) to generate random maze based on entrance and exit (aka cheese) location adapted from https://www.geeksforgeeks.org/random-acyclic-maze-generator-with-given-entry-and-exit-point/

"""

# Class to define structure of a node
class Node:
    def __init__(self, value=None, next_element=None):
        self.val = value
        self.next = next_element


# Class to implement a stack
class stack:
    # Constructor
    def __init__(self):
        self.head = None
        self.length = 0

    # Put an item on the top of the stack
    def insert(self, data):
        self.head = Node(data, self.head)
        self.length += 1

    # Return the top position of the stack
    def pop(self):
        if self.length == 0:
            return None
        else:
            returned = self.head.val
            self.head = self.head.next
            self.length -= 1
            return returned

    # Return False if the stack is empty
    # and true otherwise
    def not_empty(self):
        return bool(self.length)

    # Return the top position of the stack
    def top(self):
        return self.head.val


def generate_random_maze(
    rows: int = 4,
    columns: int = 4,
    initial_point: list = (0, 0),
    final_point: list = (3, 3),
):
    ROWS, COLS = rows, columns

    # Array with only walls (where paths will
    # be created)
    maze = list(list(0 for _ in range(COLS)) for _ in range(ROWS))

    # Auxiliary matrices to avoid cycles
    seen = list(list(False for _ in range(COLS)) for _ in range(ROWS))
    previous = list(list((-1, -1) for _ in range(COLS)) for _ in range(ROWS))

    S = stack()

    # Insert initial position
    S.insert(initial_point)

    # Keep walking on the graph using dfs
    # until we have no more paths to traverse
    # (create)
    while S.not_empty():
        # Remove the position of the Stack
        # and mark it as seen
        x, y = S.pop()
        seen[x][y] = True

        # This is to avoid cycles with adj positions
        if (x + 1 < ROWS) and maze[x + 1][y] == 1 and previous[x][y] != (x + 1, y):
            continue
        if (0 < x) and maze[x - 1][y] == 1 and previous[x][y] != (x - 1, y):
            continue
        if (y + 1 < COLS) and maze[x][y + 1] == 1 and previous[x][y] != (x, y + 1):
            continue
        if (y > 0) and maze[x][y - 1] == 1 and previous[x][y] != (x, y - 1):
            continue

        # Mark as walkable position
        maze[x][y] = 1

        # Array to shuffle neighbours before
        # insertion
        to_stack = []

        # Before inserting any position,
        # check if it is in the boundaries of
        # the maze
        # and if it were seen (to avoid cycles)

        # If adj position is valid and was not seen yet
        if (x + 1 < ROWS) and seen[x + 1][y] == False:
            # Mark the adj position as seen
            seen[x + 1][y] = True

            # Memorize the position to insert the
            # position in the stack
            to_stack.append((x + 1, y))

            # Memorize the current position as its
            # previous position on the path
            previous[x + 1][y] = (x, y)

        if (0 < x) and seen[x - 1][y] == False:
            # Mark the adj position as seen
            seen[x - 1][y] = True

            # Memorize the position to insert the
            # position in the stack
            to_stack.append((x - 1, y))

            # Memorize the current position as its
            # previous position on the path
            previous[x - 1][y] = (x, y)

        if (y + 1 < COLS) and seen[x][y + 1] == False:
            # Mark the adj position as seen
            seen[x][y + 1] = True

            # Memorize the position to insert the
            # position in the stack
            to_stack.append((x, y + 1))

            # Memorize the current position as its
            # previous position on the path
            previous[x][y + 1] = (x, y)

        if (y > 0) and seen[x][y - 1] == False:
            # Mark the adj position as seen
            seen[x][y - 1] = True

            # Memorize the position to insert the
            # position in the stack
            to_stack.append((x, y - 1))

            # Memorize the current position as its
            # previous position on the path
            previous[x][y - 1] = (x, y)

        # Indicates if Pf is a neighbour position
        pf_flag = False
        while len(to_stack):
            # Remove random position
            neighbour = to_stack.pop(randint(0, len(to_stack) - 1))

            # Is the final position,
            # remember that by marking the flag
            if neighbour == final_point:
                pf_flag = True

            # Put on the top of the stack
            else:
                S.insert(neighbour)

        # This way, Pf will be on the top
        if pf_flag:
            S.insert(final_point)

    # Mark the initial position
    x0, y0 = initial_point
    xf, yf = final_point
    maze[x0][y0] = 1
    maze[xf][yf] = 1

    # Return maze formed by the traversed path
    return np.asarray(maze, dtype="float")


# Test Run to ensure that function is working correctly
test_cols = 8
test_rows = 8
test_init_point = (0, 0)
test_final_point = (7, 7)

test_maze = generate_random_maze(
    rows=test_rows,
    columns=test_cols,
    initial_point=test_init_point,
    final_point=test_final_point,
)
# check that the shape generated is correct
assert test_maze.shape == (test_cols, test_rows)

"""### Generating a maze array and initializing a Qmaze

"""

maze = generate_random_maze(2, 2, (0, 0), (1, 1))
qmaze = Qmaze(maze=maze)
show(qmaze)

maze_2 = generate_random_maze(10, 10, (0, 0), (9, 9))
qmaze_2 = Qmaze(maze=maze_2)
show(qmaze_2)

import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical

class SerotoninSystem:
    def __init__(self, baseline=0.5, tau=0.1):
        self.baseline = baseline
        self.tau = tau  # Time constant for serotonin dynamics
        self.tonic = torch.tensor(baseline)  # Baseline serotonin level
        self.phasic = torch.tensor(0.0)      # Phasic serotonin response

    def update(self, reward_history, stress_signal):
        """
        Update serotonin levels based on reward history and stress signals.

        Parameters:
        - reward_history: List of recent rewards
        - stress_signal: Current stress level (0-1)
        """
        # Calculate average reward
        avg_reward = sum(reward_history) / len(reward_history) if len(reward_history) > 0 else 0

        # Serotonin responds positively to consistent rewards and negatively to stress
        reward_response = torch.sigmoid(torch.tensor(avg_reward * 2 - 1))  # Scale to [-1,1] then sigmoid
        stress_response = 1 - torch.sigmoid(torch.tensor(stress_signal * 3))  # Stronger response to stress

        # Phasic response combines both factors
        self.phasic = (reward_response + stress_response) / 2

        # Tonic level slowly follows phasic changes
        self.tonic = self.tonic * (1 - self.tau) + (self.baseline + self.phasic) * self.tau

        # Return total serotonin level (clamped between 0 and 1)
        return torch.clamp(self.tonic + self.phasic, 0, 1)

class BioModel(nn.Module):
    def __init__(self, maze_size, num_actions):
        super(BioModel, self).__init__()
        self.maze_size = maze_size
        self.num_actions = num_actions

        # Striatal complex (D1 and D2 pathways)
        self.striatum = nn.Sequential(
            nn.Linear(maze_size, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU()
        )

        # Serotonin modulation layer
        self.serotonin_modulation = nn.Linear(64, 64)

        # Define both pathways with serotonin sensitivity
        self.direct_pathway = nn.Sequential(
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, num_actions)
        )

        self.indirect_pathway = nn.Sequential(
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, num_actions)
        )

        self.serotonin_system = SerotoninSystem()
        self.register_buffer('serotonin', torch.tensor(0.5))

    def forward(self, x):
        if isinstance(x, np.ndarray):
            x = torch.FloatTensor(x)
        if x.dim() == 1:
            x = x.unsqueeze(0)

        # Striatal processing
        x = self.striatum(x)

        # Apply serotonin modulation
        # Serotonin enhances the indirect pathway (behavioral inhibition)
        # and modulates the direct pathway (action selection)
        modulated = torch.sigmoid(self.serotonin_modulation(x)) * (1 + self.serotonin)

        # Process through both pathways
        direct = self.direct_pathway(modulated)
        indirect = self.indirect_pathway(modulated * (1 + self.serotonin))  # Stronger effect on indirect pathway

        # Combine pathways (D1 - D2) with serotonin modulation
        q_values = direct - (indirect * (1 - self.serotonin))  # Serotonin reduces indirect pathway influence

        return q_values.squeeze(0) if q_values.size(0) == 1 else q_values

    def update_serotonin(self, reward_history, stress_signal):
        self.serotonin = self.serotonin_system.update(reward_history, stress_signal)

"""## Create a Trial

Create an `Trial` class that accepts a trained neural network which calculates the next action, a Qmaze and the initial cell that the mouse is in.

"""

class Trial:
    def __init__(self, model: BioModel, qmaze: Qmaze, mouse_cell: list):
        self._qmaze = qmaze
        self._model = model
        self.mouse_cell = mouse_cell

    def run(self):
        print("running trial...")
        mouse_cell = self._get_mouse_cell()
        self._qmaze.reset(mouse_cell)
        env_state = self._qmaze.observe()
        while True:
            prev_env_state = env_state
            Q = self._model.get_model().predict(prev_env_state)
            action = np.argmax(Q[0])
            _, _, status = self._qmaze.act(action)
            if status == "win":
                return True
            elif status == "lose":
                return False

    # For small mazes we can allow ourselves to perform a completion check in which we simulate all possible
    # games and check if our model wins the all. This is not practical for large mazes as it slows down training.
    def check(self):
        self._qmaze = self._get_maze()
        for cell in self._qmaze.free_cells:
            if not self._qmaze.valid_actions(cell):
                return False
            if not self.run():
                return False
        return True

    def _get_maze(self):
        return self._qmaze

    def _get_model(self):
        return self._model

    def _get_mouse_cell(self):
        return self.mouse_cell

"""## Creating a Class to Model the Experience of the Mouse

Create an `Experience` class that collects the experience of `Experiments` within a `list` of memory. It retreives a `model`, a `max_memory` which defines the maximum amount of experiments that the mouse can _remember_ and a `discount` factor which represents the instantanious uncertainty in the _Bellman equation for stochastic environments_.

"""

class BioExperience:
    def __init__(self, model, max_memory=8, discount=0.95):
        self.model = model
        self.max_memory = max_memory
        self.discount = discount
        self.memory = []
        self.actions = model.num_actions
        self.reward_history = []

    def remember(self, trial):
        self.memory.append(trial)
        if len(self.memory) > self.max_memory:
            del self.memory[0]

        # Keep track of recent rewards for serotonin updates
        _, _, reward, _, trial_over = trial
        if trial_over:
            self.reward_history.append(reward)
            if len(self.reward_history) > 10:  # Keep last 10 rewards
                self.reward_history.pop(0)

            # Calculate stress signal (inverse of average reward)
            stress_signal = 1.0 - (sum(self.reward_history) / len(self.reward_history)) if len(self.reward_history) > 0 else 0.5
            self.model.update_serotonin(self.reward_history, stress_signal)

    def data(self, data_size=10):
        env_size = self.memory[0][0].shape[1]
        mem_size = len(self.memory)
        data_size = min(mem_size, data_size)

        inputs = torch.zeros((data_size, env_size))
        targets = torch.zeros((data_size, self.actions))

        for idx, jdx in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):
            env_state, action, reward, env_state_next, trial_over = self.memory[jdx]

            env_state_tensor = torch.FloatTensor(env_state).view(1, -1)
            inputs[idx] = env_state_tensor

            current_q = self.model(env_state_tensor).squeeze(0).detach()
            targets[idx] = current_q.clone()

            if trial_over:
                targets[idx, action] = reward
            else:
                env_state_next_tensor = torch.FloatTensor(env_state_next).view(1, -1)
                next_q = self.model(env_state_next_tensor).squeeze(0).detach()
                targets[idx, action] = reward + self.discount * torch.max(next_q)

        return inputs, targets

"""# Q-Training Algorithm for Reinforcement Learning of Mouse
The algorithm accepts the a `number_epoch` which is the number of epochs, the maximum memory `max_memory` which is the maximum number of trials kept in memory and the `data_size` which is the number of  samples in training epoch. This is the number of trials randomly selected from the mouse's experience
"""

# This is a small utility for printing readable time strings:
class BioExperiment:
    def __init__(self, maze=generate_random_maze(), learning_rate=0.001):
        self.qmaze = Qmaze(maze)
        self.model = BioModel(maze.size, num_actions)
        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        self.trial = Trial(self.model, self.qmaze, (0, 0))

    @staticmethod
    def format_time(seconds):
        if seconds < 400:
            return f"{seconds:.1f} seconds"
        elif seconds < 4000:
            return f"{seconds/60:.2f} minutes"
        else:
            return f"{seconds/3600:.2f} hours"

    def train(self, log_file=None, **opt):
        global epsilon
        show(self.qmaze)
        print("Training biologically plausible model with serotonin modulation...")

        number_epoch = opt.get('epochs', 1000)
        max_memory = opt.get('max_memory', 8)
        data_size = opt.get('data_size', 32)
        model_name = opt.get('name', 'bio_serotonin_model')
        start_time = time.time()

        data_folder = f"experiments/{model_name}"
        os.makedirs(data_folder, exist_ok=True)

        if log_file is None:
            log_file = f"{data_folder}/{model_name}_training_metrics.csv"

        experience = BioExperience(self.model, max_memory=max_memory)
        completion_history = []
        hsize = self.qmaze.maze.size // 2
        total_wins = 0
        track_neuromod = False

        with open(log_file, mode='w', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=[
                "epoch", "loss", "episodes", "steps", "win count", "win_rate",
                "optimal_path_length", "suboptimal_steps", "total_reward", 
                "learning_rate", "elapsed_time"
            ])
            writer.writeheader()

            # Create neuromodulator log file and write header
            with open(f"{data_folder}/{model_name}_neuromodulators.csv", 'w', newline='') as nf:
                neurowriter = csv.DictWriter(nf, fieldnames=[
                    "epoch", "episode", "step", "serotonin", "reward", "status"
                ])
                neurowriter.writeheader()

                for epoch in range(number_epoch):
                    epoch_start_time = time.time()
                    total_loss = 0.0
                    wins_in_epoch = 0
                    episodes_in_epoch = 0
                    total_reward_in_epoch = 0
                    total_steps_in_epoch = 0

                    for episode in range(data_size):
                        mouse_cell = random.choice(self.qmaze.free_cells)
                        self.qmaze.reset(mouse_cell)
                        env_state = self.qmaze.observe()
                        trial_over = False
                        episode_reward = 0
                        steps_in_episode = 0

                        while not trial_over:
                            valid_actions = self.qmaze.valid_actions()
                            if not valid_actions:
                                break

                            prev_env_state = env_state

                            # Serotonin-modulated exploration
                            # Higher serotonin makes agent more cautious (less random exploration)
                            if np.random.rand() < epsilon * (1 - self.model.serotonin.item()):
                                action = random.choice(valid_actions)
                            else:
                                with torch.no_grad():
                                    q_values = self.model(torch.FloatTensor(prev_env_state).view(1, -1))
                                    action = torch.argmax(q_values).item()

                            env_state, reward, status = self.qmaze.act(action)
                            episode_reward += reward
                            trial_over = (status != 'not_over')

                            experience.remember((prev_env_state, action, reward, env_state, trial_over))

                            if trial_over:
                                episodes_in_epoch += 1
                                if status == "win":
                                    wins_in_epoch += 1
                                    total_wins += 1
                                completion_history.append(1 if status == "win" else 0)

                            if track_neuromod:
                                neurowriter.writerow({
                                    "epoch": epoch,
                                    "episode": episode,
                                    "step": steps_in_episode,
                                    "serotonin": self.model.serotonin.item(),
                                    "reward": reward,
                                    "status": status
                                })

                            steps_in_episode += 1

                        total_reward_in_epoch += episode_reward

                    # Learning update after each episode
                    inputs, targets = experience.data(data_size=data_size)
                    self.optimizer.zero_grad()
                    outputs = self.model(inputs)
                    loss = nn.MSELoss()(outputs, targets.squeeze())
                    loss.backward()

                    # Serotonin-modulated learning
                    # Higher serotonin strengthens learning in indirect pathway
                    for name, param in self.model.named_parameters():
                        if 'direct_pathway' in name and param.grad is not None:
                            param.grad *= (1 + 0.5 * self.model.serotonin.item())  # Moderate effect
                        elif 'indirect_pathway' in name and param.grad is not None:
                            param.grad *= (1 + self.model.serotonin.item())  # Stronger effect

                    self.optimizer.step()
                    total_loss += loss.item()

                    # Calculate win rate and statistics
                    win_rate = wins_in_epoch / episodes_in_epoch if episodes_in_epoch > 0 else 0
                    total_time = time.time() - start_time
                    avg_loss = total_loss / episodes_in_epoch if episodes_in_epoch > 0 else 0

                    try:
                        optimal_path = self.qmaze.shortest_path(mouse_cell)
                        optimal_path_length = len(optimal_path)
                    except:
                        optimal_path_length = -1  # fallback if path not found

                    suboptimal_steps = max(0, episodes_in_epoch - optimal_path_length)

                    learning_rate = self.optimizer.param_groups[0]['lr']

                    # log metrics to csv
                    writer.writerow({
                        "epoch": epoch,
                        "loss": avg_loss,
                        "episodes": episodes_in_epoch,
                        "steps": total_steps_in_epoch,
                        "win count": wins_in_epoch,
                        "win_rate": win_rate,
                        "optimal_path_length": optimal_path_length,
                        "suboptimal_steps": suboptimal_steps,
                        "total_reward": total_reward_in_epoch,
                        "learning_rate": learning_rate,
                        "elapsed_time": total_time
                    })

                    # Print serotonin level for monitoring
                    serotonin_level = self.model.serotonin.item()
                    print(f"Epoch: {epoch:03d}/{number_epoch - 1} | "
                        f"Loss: {avg_loss:.4f} | "
                        f"Episodes: {episodes_in_epoch} | "
                        f"Win count: {total_wins} | "
                        f"Win rate: {win_rate:.3f} | "
                        f"Serotonin: {serotonin_level:.3f} | "
                        f"time: {total_time:.1f} seconds")

                    # Early stopping
                    if win_rate > 0.9:
                        epsilon = max(0.05, epsilon * 0.99)  # Gradually reduce exploration
                    if win_rate == 1.0 and episodes_in_epoch >= 10:
                        print(f"Reached 100% episode win rate at epoch: {epoch}")
                        break

        torch.save(self.model.state_dict(), f"{model_name}.pth")
        final_time = time.time() - start_time
        print(f"Model saved to {model_name}.pth")
        print(f"Training completed in {final_time:.1f} seconds")
        print(f"Final win count: {total_wins} out of {number_epoch * data_size} episodes")
        return final_time

"""## Train the Model"""

# Run the experiment
if __name__ == "__main__":
    for i in range(1, 6):
        name = f"serotonin_{i:02d}"
        
        # load the pre-generated maze
        maze_path = f"experiments/mazes_5x5/maze_{i}.npy"  # adjust path if needed
        maze = np.load(maze_path)

        # create a new instance of Experiment with the loaded maze
        experiment = BioExperiment(maze, learning_rate=0.001)

        print(f"Running experiment: {name}")
        experiment.train(epochs=1000, max_memory=8, data_size=100, name=name)
